{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bb38d2-d1f5-4db4-ab04-ec01ff520f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "|           datetime|season|holiday|workingday|weather| temp| atemp|humidity|windspeed|casual|registered|count|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "|2011-01-01 00:00:00|     1|      0|         0|      1| 9.84|14.395|      81|      0.0|     3|        13|   16|\n",
      "|2011-01-01 01:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     8|        32|   40|\n",
      "|2011-01-01 02:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     5|        27|   32|\n",
      "|2011-01-01 03:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     3|        10|   13|\n",
      "|2011-01-01 04:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     0|         1|    1|\n",
      "|2011-01-01 05:00:00|     1|      0|         0|      2| 9.84| 12.88|      75|   6.0032|     0|         1|    1|\n",
      "|2011-01-01 06:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     2|         0|    2|\n",
      "|2011-01-01 07:00:00|     1|      0|         0|      1|  8.2| 12.88|      86|      0.0|     1|         2|    3|\n",
      "|2011-01-01 08:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     1|         7|    8|\n",
      "|2011-01-01 09:00:00|     1|      0|         0|      1|13.12|17.425|      76|      0.0|     8|         6|   14|\n",
      "|2011-01-01 10:00:00|     1|      0|         0|      1|15.58|19.695|      76|  16.9979|    12|        24|   36|\n",
      "|2011-01-01 11:00:00|     1|      0|         0|      1|14.76|16.665|      81|  19.0012|    26|        30|   56|\n",
      "|2011-01-01 12:00:00|     1|      0|         0|      1|17.22| 21.21|      77|  19.0012|    29|        55|   84|\n",
      "|2011-01-01 13:00:00|     1|      0|         0|      2|18.86|22.725|      72|  19.9995|    47|        47|   94|\n",
      "|2011-01-01 14:00:00|     1|      0|         0|      2|18.86|22.725|      72|  19.0012|    35|        71|  106|\n",
      "|2011-01-01 15:00:00|     1|      0|         0|      2|18.04| 21.97|      77|  19.9995|    40|        70|  110|\n",
      "|2011-01-01 16:00:00|     1|      0|         0|      2|17.22| 21.21|      82|  19.9995|    41|        52|   93|\n",
      "|2011-01-01 17:00:00|     1|      0|         0|      2|18.04| 21.97|      82|  19.0012|    15|        52|   67|\n",
      "|2011-01-01 18:00:00|     1|      0|         0|      3|17.22| 21.21|      88|  16.9979|     9|        26|   35|\n",
      "|2011-01-01 19:00:00|     1|      0|         0|      3|17.22| 21.21|      88|  16.9979|     6|        31|   37|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weather: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- casual: integer (nullable = true)\n",
      " |-- registered: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadFromHDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Specify the HDFS path to your data\n",
    "hdfs_path = \"hdfs://namenode_host:9870/user/root/input/train.csv\"\n",
    "\n",
    "# Read data into a DataFrame (assuming the data is in CSV format)\n",
    "df = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68994778-5b15-4e3c-a5ff-94b8b109e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(datetime='2011-01-01 00:00:00', season=1, holiday=0, workingday=0, weather=1, temp=9.84, atemp=14.395, humidity=81, windspeed=0.0, casual=3, registered=13, count=16),\n",
       " Row(datetime='2011-01-01 01:00:00', season=1, holiday=0, workingday=0, weather=1, temp=9.02, atemp=13.635, humidity=80, windspeed=0.0, casual=8, registered=32, count=40),\n",
       " Row(datetime='2011-01-01 02:00:00', season=1, holiday=0, workingday=0, weather=1, temp=9.02, atemp=13.635, humidity=80, windspeed=0.0, casual=5, registered=27, count=32),\n",
       " Row(datetime='2011-01-01 03:00:00', season=1, holiday=0, workingday=0, weather=1, temp=9.84, atemp=14.395, humidity=75, windspeed=0.0, casual=3, registered=10, count=13),\n",
       " Row(datetime='2011-01-01 04:00:00', season=1, holiday=0, workingday=0, weather=1, temp=9.84, atemp=14.395, humidity=75, windspeed=0.0, casual=0, registered=1, count=1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ebcf65-ecef-4022-b9d5-7e96c45c9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"bike_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbca3cae-252b-49ea-b88f-fa4c37364347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "|           datetime|season|holiday|workingday|weather| temp| atemp|humidity|windspeed|casual|registered|count|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "|2011-01-01 00:00:00|     1|      0|         0|      1| 9.84|14.395|      81|      0.0|     3|        13|   16|\n",
      "|2011-01-01 01:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     8|        32|   40|\n",
      "|2011-01-01 02:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     5|        27|   32|\n",
      "|2011-01-01 03:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     3|        10|   13|\n",
      "|2011-01-01 04:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     0|         1|    1|\n",
      "|2011-01-01 05:00:00|     1|      0|         0|      2| 9.84| 12.88|      75|   6.0032|     0|         1|    1|\n",
      "|2011-01-01 06:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     2|         0|    2|\n",
      "|2011-01-01 07:00:00|     1|      0|         0|      1|  8.2| 12.88|      86|      0.0|     1|         2|    3|\n",
      "|2011-01-01 08:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     1|         7|    8|\n",
      "|2011-01-01 09:00:00|     1|      0|         0|      1|13.12|17.425|      76|      0.0|     8|         6|   14|\n",
      "|2011-01-01 10:00:00|     1|      0|         0|      1|15.58|19.695|      76|  16.9979|    12|        24|   36|\n",
      "|2011-01-01 11:00:00|     1|      0|         0|      1|14.76|16.665|      81|  19.0012|    26|        30|   56|\n",
      "|2011-01-01 12:00:00|     1|      0|         0|      1|17.22| 21.21|      77|  19.0012|    29|        55|   84|\n",
      "|2011-01-01 13:00:00|     1|      0|         0|      2|18.86|22.725|      72|  19.9995|    47|        47|   94|\n",
      "|2011-01-01 14:00:00|     1|      0|         0|      2|18.86|22.725|      72|  19.0012|    35|        71|  106|\n",
      "|2011-01-01 15:00:00|     1|      0|         0|      2|18.04| 21.97|      77|  19.9995|    40|        70|  110|\n",
      "|2011-01-01 16:00:00|     1|      0|         0|      2|17.22| 21.21|      82|  19.9995|    41|        52|   93|\n",
      "|2011-01-01 17:00:00|     1|      0|         0|      2|18.04| 21.97|      82|  19.0012|    15|        52|   67|\n",
      "|2011-01-01 18:00:00|     1|      0|         0|      3|17.22| 21.21|      88|  16.9979|     9|        26|   35|\n",
      "|2011-01-01 19:00:00|     1|      0|         0|      3|17.22| 21.21|      88|  16.9979|     6|        31|   37|\n",
      "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bs_df = spark.sql(\"select * from bike_train\")\n",
    "bs_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fea5848-9e45-494e-a3dd-ca744b76a266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|           datetime|            season|            holiday|        workingday|           weather|              temp|            atemp|          humidity|         windspeed|           casual|        registered|             count|\n",
      "+-------+-------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|              10886|             10886|              10886|             10886|             10886|             10886|            10886|             10886|             10886|            10886|             10886|             10886|\n",
      "|   mean|               null|2.5066139996325556|0.02856880396839978|0.6808745177291935| 1.418427337865148|20.230859819952173|23.65508405291192| 61.88645967297446|12.799395406945093|36.02195480433584| 155.5521771082124|191.57413191254824|\n",
      "| stddev|               null|1.1161743093443237|0.16659885062470944|0.4661591687997361|0.6338385858190968| 7.791589843987573| 8.47460062648494|19.245033277394704|  8.16453732683871|49.96047657264955|151.03903308192452|181.14445383028493|\n",
      "|    min|2011-01-01 00:00:00|                 1|                  0|                 0|                 1|              0.82|             0.76|                 0|               0.0|                0|                 0|                 1|\n",
      "|    max|2012-12-19 23:00:00|                 4|                  1|                 1|                 4|              41.0|           45.455|               100|           56.9969|              367|               886|               977|\n",
      "+-------+-------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bs_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5e1efb2-b013-495e-a6e1-4c3f02784d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [datetime#158,season#159,holiday#160,workingday#161,weather#162,temp#163,atemp#164,humidity#165,windspeed#166,casual#167,registered#168,count#169] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/opt/spark/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<datetime:string,season:int,holiday:int,workingday:int,weather:int,temp:double,atemp:double...\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(bs_df.explain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f10449-c352-4674-a2d8-7f627b30b1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10886\n",
      "10886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(bs_df.count())\n",
    "df_no_null = bs_df.na.drop()\n",
    "print(df_no_null.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834a564e-7ba2-422b-8a61-9772de6ffcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|season|\n",
      "+------+\n",
      "|     1|\n",
      "|     3|\n",
      "|     4|\n",
      "|     2|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|weather|\n",
      "+-------+\n",
      "|      1|\n",
      "|      3|\n",
      "|      4|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bs_df.select('season').distinct().show()\n",
    "bs_df.select('weather').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e222537f-b85d-48d5-a37a-79613c870a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns encoding\n",
    "def valueToCategory(value, encoding_index):\n",
    "   if(value == encoding_index):\n",
    "      return 1\n",
    "   else:\n",
    "    return 0\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceb342b8-eebf-4af8-903f-250beaaaecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "udfValueToCategory = udf(valueToCategory, IntegerType())\n",
    "bs_df_encoded = (bs_df.withColumn(\"season_1\", udfValueToCategory(col('season'),lit(1)))\n",
    "                     .withColumn(\"season_2\", udfValueToCategory(col('season'),lit(2)))\n",
    "                     .withColumn(\"season_3\", udfValueToCategory(col('season'),lit(3)))\n",
    "                     .withColumn(\"season_4\", udfValueToCategory(col('season'),lit(4))))\n",
    "bs_df_encoded = bs_df_encoded.drop('season')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a458c52e-2ba7-4815-996c-f605e96a4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bs_df_encoded = (bs_df_encoded.withColumn(\"weather_1\", udfValueToCategory(col('weather'),lit(1)))\n",
    "                     .withColumn(\"weather_2\", udfValueToCategory(col('weather'),lit(2)))\n",
    "                     .withColumn(\"weather_3\", udfValueToCategory(col('weather'),lit(3)))\n",
    "                     .withColumn(\"weather_4\", udfValueToCategory(col('weather'),lit(4))))\n",
    "bs_df_encoded = bs_df_encoded.drop('weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be038fa-7583-4a97-bc01-efe934c962c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "|datetime           |holiday|workingday|temp|atemp |humidity|windspeed|casual|registered|count|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|\n",
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "|2011-01-01 00:00:00|0      |0         |9.84|14.395|81      |0.0      |3     |13        |16   |1       |0       |0       |0       |1        |0        |0        |0        |\n",
      "|2011-01-01 01:00:00|0      |0         |9.02|13.635|80      |0.0      |8     |32        |40   |1       |0       |0       |0       |1        |0        |0        |0        |\n",
      "|2011-01-01 02:00:00|0      |0         |9.02|13.635|80      |0.0      |5     |27        |32   |1       |0       |0       |0       |1        |0        |0        |0        |\n",
      "|2011-01-01 03:00:00|0      |0         |9.84|14.395|75      |0.0      |3     |10        |13   |1       |0       |0       |0       |1        |0        |0        |0        |\n",
      "|2011-01-01 04:00:00|0      |0         |9.84|14.395|75      |0.0      |0     |1         |1    |1       |0       |0       |0       |1        |0        |0        |0        |\n",
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bs_df_encoded.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3690cbe-88a5-40dd-baff-30eef84b86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "bs_df_encoded = bs_df_encoded.withColumn('hour',  split(split(bs_df_encoded['datetime'], ' ')[1], ':')[0].cast('int'))\n",
    "bs_df_encoded = bs_df_encoded.withColumn('month', split(split(bs_df_encoded['datetime'], ' ')[0], '-')[0].cast('int'))\n",
    "bs_df_encoded = bs_df_encoded.withColumn('day', split(split(bs_df_encoded['datetime'], ' ')[0], '-')[1].cast('int'))\n",
    "bs_df_encoded = bs_df_encoded.withColumn('year', split(split(bs_df_encoded['datetime'], ' ')[0], '-')[2].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfa1cd5e-9a2e-4e38-959e-ef4623b10432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+\n",
      "|datetime           |holiday|workingday|temp|atemp |humidity|windspeed|casual|registered|count|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|hour|month|day|year|\n",
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+\n",
      "|2011-01-01 00:00:00|0      |0         |9.84|14.395|81      |0.0      |3     |13        |16   |1       |0       |0       |0       |1        |0        |0        |0        |0   |2011 |1  |1   |\n",
      "|2011-01-01 01:00:00|0      |0         |9.02|13.635|80      |0.0      |8     |32        |40   |1       |0       |0       |0       |1        |0        |0        |0        |1   |2011 |1  |1   |\n",
      "|2011-01-01 02:00:00|0      |0         |9.02|13.635|80      |0.0      |5     |27        |32   |1       |0       |0       |0       |1        |0        |0        |0        |2   |2011 |1  |1   |\n",
      "|2011-01-01 03:00:00|0      |0         |9.84|14.395|75      |0.0      |3     |10        |13   |1       |0       |0       |0       |1        |0        |0        |0        |3   |2011 |1  |1   |\n",
      "|2011-01-01 04:00:00|0      |0         |9.84|14.395|75      |0.0      |0     |1         |1    |1       |0       |0       |0       |1        |0        |0        |0        |4   |2011 |1  |1   |\n",
      "+-------------------+-------+----------+----+------+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bs_df_encoded.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfcf06ba-b365-453f-a58d-66234b4ea1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_df_encoded = bs_df_encoded.drop('datetime')\n",
    "bs_df_encoded = bs_df_encoded.withColumnRenamed(\"count\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25a51b9-c203-489b-9eac-b383244d9b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- casual: integer (nullable = true)\n",
      " |-- registered: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- season_1: integer (nullable = true)\n",
      " |-- season_2: integer (nullable = true)\n",
      " |-- season_3: integer (nullable = true)\n",
      " |-- season_4: integer (nullable = true)\n",
      " |-- weather_1: integer (nullable = true)\n",
      " |-- weather_2: integer (nullable = true)\n",
      " |-- weather_3: integer (nullable = true)\n",
      " |-- weather_4: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bs_df_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10c9dc31-2ece-4bec-94a9-e22c3ee58813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML LIB TRAIN and TEST split is done as follows, 80% for training and 20% for testing\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "train, test = bs_df_encoded.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0f934ae-cd3d-4fdb-9734-1c7f7e406799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 06:40:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+--------------------+\n",
      "|holiday|workingday|temp|atemp|humidity|windspeed|casual|registered|label|season_1|season_2|season_3|season_4|weather_1|weather_2|weather_3|weather_4|hour|month|day|year|            features|\n",
      "+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+--------------------+\n",
      "|      0|         0|3.28|2.275|      79|  31.0009|     0|        24|   24|       1|       0|       0|       0|        0|        0|        1|        0|   1| 2012|  2|  12|(21,[2,3,4,5,7,8,...|\n",
      "|      0|         0|3.28| 3.79|      53|  16.9979|     0|        26|   26|       1|       0|       0|       0|        1|        0|        0|        0|   8| 2012|  2|  12|(21,[2,3,4,5,7,8,...|\n",
      "|      0|         0|3.28|4.545|      53|   12.998|     0|         1|    1|       1|       0|       0|       0|        1|        0|        0|        0|   4| 2011|  1|   9|(21,[2,3,4,5,7,8,...|\n",
      "|      0|         0|3.28|4.545|      53|   12.998|     0|         1|    1|       1|       0|       0|       0|        1|        0|        0|        0|   5| 2011|  1|   9|(21,[2,3,4,5,7,8,...|\n",
      "|      0|         0|3.28|4.545|      53|   12.998|     0|        18|   18|       1|       0|       0|       0|        1|        0|        0|        0|   7| 2012|  2|  12|(21,[2,3,4,5,7,8,...|\n",
      "+-------+----------+----+-----+--------+---------+------+----------+-----+--------+--------+--------+--------+---------+---------+---------+---------+----+-----+---+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "8694\n",
      "8694\n"
     ]
    }
   ],
   "source": [
    "#defining input variables and output\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"holiday\",\"workingday\",\"temp\",\"atemp\",\"humidity\",\"windspeed\",\"casual\",\"registered\",\"label\",\"season_1\",\"season_2\",\"season_3\",\"season_4\",\"weather_1\",\"weather_2\",\"weather_3\",\"weather_4\", \"hour\", \"month\", \"day\", \"year\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(train)\n",
    "#Assembled columns 'hour', 'day' etc  to vector column 'features'\n",
    "output.show(5)\n",
    "print(output.count())\n",
    "#setting up TRAINing data\n",
    "train_output = output.na.drop()\n",
    "print(train_output.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a3dcee3-bfac-4136-b964-c927e862e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                           |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "|(21,[2,3,4,5,7,8,9,15,17,18,19,20],[3.28,2.275,79.0,31.0009,24.0,24.0,1.0,1.0,1.0,2012.0,2.0,12.0])|\n",
      "|(21,[2,3,4,5,7,8,9,13,17,18,19,20],[3.28,3.79,53.0,16.9979,26.0,26.0,1.0,1.0,8.0,2012.0,2.0,12.0]) |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('features').show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5950087-5a9e-4ea2-9cb3-6c4e00b33b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2192\n",
      "2192\n"
     ]
    }
   ],
   "source": [
    "# setting up testing data\n",
    "\n",
    "test_output = assembler.transform(test)\n",
    "print(test_output.count())\n",
    "train_output = test_output.na.drop()\n",
    "print(test_output.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5569193-75c8-40bf-89c4-7c317adc376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 06:41:10 WARN Instrumentation: [9b438f12] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/09 06:41:10 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/09 06:41:10 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "24/08/09 06:41:11 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/08/09 06:41:11 WARN Instrumentation: [9b438f12] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce620c5b-80ea-4971-9c38-e2c67e3f1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.07368046826033069,0.049734509565215995,0.0041106952264321935,-0.004801552301310095,-0.0010787615437215469,-0.005644245537650441,0.5646486605063272,0.5644864723773111,0.43543779135894456,0.19539787908072534,0.16734078392296084,-0.040642909955340714,-0.33123405748358803,-0.11704541232870219,0.08902397276438985,0.14380537145727326,-1.1158662113046425,0.005969780356600364,-0.07566503424084421,0.0503776787076451,0.0015697279247076968]\n",
      "\n",
      "\n",
      "Intercept: 151.9713713469821\n"
     ]
    }
   ],
   "source": [
    "#Coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"\\n\")\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cc90730-5385-43ee-a319-9f3dec911e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(21,[2,3,4,5,7,8,...|   14|13.850845007997435|\n",
      "|[0.0,0.0,4.92,5.3...|  124|123.82459217736175|\n",
      "|[0.0,0.0,4.92,5.3...|   91| 90.79935621662776|\n",
      "|(21,[2,3,4,5,7,8,...|    6| 5.948871769180414|\n",
      "|(21,[2,3,4,6,7,8,...|   10| 9.911198997576037|\n",
      "|[0.0,0.0,5.74,6.0...|   15| 14.91484811397288|\n",
      "|(21,[2,3,4,5,7,8,...|   23| 22.92662106474387|\n",
      "|[0.0,0.0,6.56,6.0...|   49| 48.77129125145997|\n",
      "|(21,[2,3,4,5,6,7,...|   44|43.774120844042116|\n",
      "|[0.0,0.0,6.56,9.0...|   43|42.921706178117645|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions\n",
    "predictions = lrModel.transform(test_output).select(\"features\", \"label\", \"prediction\")\n",
    "predictions.show(10)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "# testRDD = test.rdd \n",
    "# predictionAndLabels = testRDD.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "# # Evaluate model\n",
    "# metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "# f1Score = metrics.fMeasure()\n",
    "# print(f1Score)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\",metricName=\"r2\")\n",
    "# print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c2c1033-6d35-469d-abfc-1834c7e9f88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+\n",
      "|            features|        prediction|label|\n",
      "+--------------------+------------------+-----+\n",
      "|(21,[2,3,4,5,7,8,...|13.348488497640506|   14|\n",
      "|[0.0,0.0,4.92,5.3...|119.68936724561414|  124|\n",
      "|[0.0,0.0,4.92,5.3...| 94.73015986386835|   91|\n",
      "|(21,[2,3,4,5,7,8,...| 29.36962517840549|    6|\n",
      "|(21,[2,3,4,6,7,8,...|14.168208513603483|   10|\n",
      "+--------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Forest Classifier model\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "rf_model = rf.fit(train_output)\n",
    "\n",
    "predictions = rf_model.transform(test_output)\n",
    "\n",
    "display(predictions.select( \"features\",\"prediction\", \"label\").show(5))\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "#evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "#rmse = evaluator.evaluate(predictions)\n",
    "#print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "144e5978-cd5b-4a5e-8704-43534d674870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_model.save(\"saved_model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91be3def-1123-4e46-9a14-faa564750c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "loaded_model = RandomForestRegressionModel.load(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8674718-16ee-4361-b69d-10b02d6c5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f52b6db3-b1bf-4ece-8da2-b612186d8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+\n",
      "|            features|        prediction|label|\n",
      "+--------------------+------------------+-----+\n",
      "|(21,[2,3,4,5,7,8,...|13.348488497640506|   14|\n",
      "|[0.0,0.0,4.92,5.3...|119.68936724561414|  124|\n",
      "|[0.0,0.0,4.92,5.3...| 94.73015986386835|   91|\n",
      "|(21,[2,3,4,5,7,8,...| 29.36962517840549|    6|\n",
      "|(21,[2,3,4,6,7,8,...|14.168208513603483|   10|\n",
      "+--------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.transform(test_output)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select( \"features\",\"prediction\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df2d04-bb90-4b0a-b1dc-f8696bb62274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
